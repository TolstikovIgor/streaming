igor@igor-MS-7808:~$ ssh -i ~/.ssh/id_rsa_student898_2 student898_2@37.139.41.176
Last login: Sat Jan 15 20:06:42 2022 from 109-252-20-121.nat.spd-mgts.ru
[student898_2@bigdataanalytics-worker-3 ~]$ export SPARK_KAFKA_VERSION=0.10
[student898_2@bigdataanalytics-worker-3 ~]$ /opt/spark-2.4.8/bin/pyspark --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5 --driver-memory 512m --master local[1]
Python 2.7.5 (default, Nov 16 2020, 22:23:17) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Ivy Default Cache set to: /home/student898_2/.ivy2/cache
The jars for the packages stored in: /home/student898_2/.ivy2/jars
:: loading settings :: url = jar:file:/opt/spark-2.4.8/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.apache.spark#spark-sql-kafka-0-10_2.11 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-dae93edc-4807-4722-aa10-5930ec944173;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.11;2.4.5 in central
	found org.apache.kafka#kafka-clients;2.0.0 in central
	found org.lz4#lz4-java;1.4.0 in central
	found org.xerial.snappy#snappy-java;1.1.7.3 in central
	found org.slf4j#slf4j-api;1.7.16 in central
	found org.spark-project.spark#unused;1.0.0 in central
:: resolution report :: resolve 408ms :: artifacts dl 8ms
	:: modules in use:
	org.apache.kafka#kafka-clients;2.0.0 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.11;2.4.5 from central in [default]
	org.lz4#lz4-java;1.4.0 from central in [default]
	org.slf4j#slf4j-api;1.7.16 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.xerial.snappy#snappy-java;1.1.7.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-dae93edc-4807-4722-aa10-5930ec944173
	confs: [default]
	0 artifacts copied, 6 already retrieved (0kB/7ms)
22/01/16 07:46:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.8
      /_/

Using Python version 2.7.5 (default, Nov 16 2020 22:23:17)
SparkSession available as 'spark'.
>>> from pyspark.sql import functions as F
>>> from pyspark.sql.types import StructType, StringType, FloatType
>>> kafka_brokers = "bigdataanalytics-worker-3:6667"
>>> raw_data = spark.readStream. \
...     format("kafka"). \
...     option("kafka.bootstrap.servers", kafka_brokers). \
...     option("subscribe", "shadrin_iris"). \
...     option("startingOffsets", "earliest"). \
...     option("maxOffsetsPerTrigger", "5"). \
...     load()
>>> raw_data = spark.readStream. \
...     format("kafka"). \
...     option("kafka.bootstrap.servers", kafka_brokers). \
...     option("subscribe", "shadrin_data"). \
...     option("startingOffsets", "earliest"). \
...     option("maxOffsetsPerTrigger", "5"). \
...     load()
>>> schema = StructType() \
...     .add("time_id", StringType()) \
...     .add("ping_ms", StringType()) \
...     .add("temperature_c", StringType()) \
...     .add("humidity_p", StringType())
>>> parsed_data = raw_data \
...          .select(F.from_json(F.col("value").cast("String"), schema).alias("value"), "offset") \
...     .select("value.*", "offset")
>>> parsed_data.printSchema()
root
 |-- time_id: string (nullable = true)
 |-- ping_ms: string (nullable = true)
 |-- temperature_c: string (nullable = true)
 |-- humidity_p: string (nullable = true)
 |-- offset: long (nullable = true)

>>> raw_data.printSchema()
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)

>>> def console_output(df, freq):
...     return df.writeStream \
...         .format("console") \
...         .trigger(processingTime='%s seconds' % freq) \
...         .option("truncate",False) \
...         .start()
... 
>>> out = console_output(parsed_data, 5)
22/01/16 08:22:58 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
-------------------------------------------                                     
Batch: 0
-------------------------------------------
+-------+-------+-------------+----------+------+
|time_id|ping_ms|temperature_c|humidity_p|offset|
+-------+-------+-------------+----------+------+
|null   |null   |null         |null      |0     |
|null   |null   |null         |null      |1     |
|null   |null   |null         |null      |2     |
|null   |null   |null         |null      |3     |
|null   |null   |null         |null      |4     |
+-------+-------+-------------+----------+------+

22/01/16 08:23:05 WARN streaming.ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 5886 milliseconds
-------------------------------------------
Batch: 1
-------------------------------------------
+-------+-------+-------------+----------+------+
|time_id|ping_ms|temperature_c|humidity_p|offset|
+-------+-------+-------------+----------+------+
|null   |null   |null         |null      |5     |
|null   |null   |null         |null      |6     |
|null   |null   |null         |null      |7     |
|null   |null   |null         |null      |8     |
|null   |null   |null         |null      |9     |
+-------+-------+-------------+----------+------+

out.stop()
>>> out.stop()
>>> schema = StructType() \
...     .add("time_id", StringType()) \
...     .add("ping_ms", FloatType()) \
...     .add("temperature_c", FloatType()) \
...     .add("humidity_p", FloatType())
>>> parsed_data = raw_data \
...          .select(F.from_json(F.col("value").cast("String"), schema).alias("value"), "offset") \
...     .select("value.*", "offset")
>>> parsed_data = raw_data \
...     .select(F.from_json(F.col("value").cast("String"), schema).alias("value"), "offset") \
...     .select("value.*", "offset")
>>> parsed_data.printSchema()
root
 |-- time_id: string (nullable = true)
 |-- ping_ms: float (nullable = true)
 |-- temperature_c: float (nullable = true)
 |-- humidity_p: float (nullable = true)
 |-- offset: long (nullable = true)

>>> raw_data.printSchema()
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)

>>> def console_output(df, freq):
...     return df.writeStream \
...         .format("console") \
...         .trigger(processingTime='%s seconds' % freq) \
...         .option("truncate",False) \
...         .start()
... 
>>> out = console_output(parsed_data, 5)
>>> -------------------------------------------
Batch: 0
-------------------------------------------
+-------+-------+-------------+----------+------+
|time_id|ping_ms|temperature_c|humidity_p|offset|
+-------+-------+-------------+----------+------+
|null   |null   |null         |null      |0     |
|null   |null   |null         |null      |1     |
|null   |null   |null         |null      |2     |
|null   |null   |null         |null      |3     |
|null   |null   |null         |null      |4     |
+-------+-------+-------------+----------+------+

-------------------------------------------
Batch: 1
-------------------------------------------
+-------+-------+-------------+----------+------+
|time_id|ping_ms|temperature_c|humidity_p|offset|
+-------+-------+-------------+----------+------+
|null   |null   |null         |null      |5     |
|null   |null   |null         |null      |6     |
|null   |null   |null         |null      |7     |
|null   |null   |null         |null      |8     |
|null   |null   |null         |null      |9     |
+-------+-------+-------------+----------+------+

-------------------------------------------
Batch: 2
-------------------------------------------
+-------+-------+-------------+----------+------+
|time_id|ping_ms|temperature_c|humidity_p|offset|
+-------+-------+-------------+----------+------+
|null   |null   |null         |null      |10    |
|null   |null   |null         |null      |11    |
|null   |null   |null         |null      |12    |
|null   |null   |null         |null      |13    |
|null   |null   |null         |null      |14    |
+-------+-------+-------------+----------+------+

>>> -------------------------------------------
Batch: 3
-------------------------------------------
+-------+-------+-------------+----------+------+
|time_id|ping_ms|temperature_c|humidity_p|offset|
+-------+-------+-------------+----------+------+
|null   |null   |null         |null      |15    |
|null   |null   |null         |null      |16    |
|null   |null   |null         |null      |17    |
|null   |null   |null         |null      |18    |
|null   |null   |null         |null      |19    |
+-------+-------+-------------+----------+------+

out.stop()-------------------------------------------
Batch: 4
-------------------------------------------
+-------+-------+-------------+----------+------+
|time_id|ping_ms|temperature_c|humidity_p|offset|
+-------+-------+-------------+----------+------+
|null   |null   |null         |null      |20    |
|null   |null   |null         |null      |21    |
|null   |null   |null         |null      |22    |
|null   |null   |null         |null      |23    |
|null   |null   |null         |null      |24    |
+-------+-------+-------------+----------+------+

out.stop()
  File "<stdin>", line 1
    out.stop()out.stop()
                ^
SyntaxError: invalid syntax
>>> out.stop()
>>> def memory_sink(df, freq):
...     return df.writeStream.format("memory") \
...             .queryName("my_memory_sink_table") \
...             .trigger(processingTime='%s seconds' % freq) \
...             .start()
... 
>>> stream = memory_sink(parsed_data, 10)
>>> spark.sql("select * from my_memory_sink_table").show()
+-------+-------+-------------+----------+------+
|time_id|ping_ms|temperature_c|humidity_p|offset|
+-------+-------+-------------+----------+------+
|   null|   null|         null|      null|     0|
|   null|   null|         null|      null|     1|
|   null|   null|         null|      null|     2|
|   null|   null|         null|      null|     3|
|   null|   null|         null|      null|     4|
|   null|   null|         null|      null|     5|
|   null|   null|         null|      null|     6|
|   null|   null|         null|      null|     7|
|   null|   null|         null|      null|     8|
|   null|   null|         null|      null|     9|
|   null|   null|         null|      null|    10|
|   null|   null|         null|      null|    11|
|   null|   null|         null|      null|    12|
|   null|   null|         null|      null|    13|
|   null|   null|         null|      null|    14|
|   null|   null|         null|      null|    15|
|   null|   null|         null|      null|    16|
|   null|   null|         null|      null|    17|
|   null|   null|         null|      null|    18|
|   null|   null|         null|      null|    19|
+-------+-------+-------------+----------+------+
only showing top 20 rows

>>> spark.sql('select count(*) from my_memory_sink_table').show()
+--------+
|count(1)|
+--------+
|      70|
+--------+

>>> spark.sql('select count(*) from my_memory_sink_table').show()
+--------+
|count(1)|
+--------+
|      90|
+--------+

>>> stream.stop()
>>> spark.sql('select count(*) from my_memory_sink_table').show()
+--------+
|count(1)|
+--------+
|     110|
+--------+

>>> spark.sql('select count(*) from my_memory_sink_table').show()
+--------+
|count(1)|
+--------+
|     110|
+--------+

>>> spark.sql('select count(*) from my_memory_sink_table').show()
+--------+
|count(1)|
+--------+
|     110|
+--------+

>>> spark.sql('select count(*) from my_memory_sink_table').show()
+--------+
|count(1)|
+--------+
|     110|
+--------+

>>> def file_sink(df, freq):
...     return df.writeStream.format("parquet") \
...             .trigger(processingTime='%s seconds' % freq) \
...             .option("path", "my_parquet_sink") \
...             .option("checkpointLocation", "shadrin_data_file_checkpoint") \
...             .start()
... 
>>> stream = file_sink(parsed_data, 5)
stream.stop()                                                                   
>>> stream.stop()
>>> def kafka_sink(df, freq):
...     return df.selectExpr("CAST(null AS STRING) as key", "CAST(struct(*) AS STRING) as value") \
...             .writeStream \
...             .format("kafka") \
...             .trigger(processingTime='%s seconds' % freq) \
...             .option("topic", "shadrin_data_sink") \
...             .option("kafka.bootstrap.servers", kafka_brokers) \
...             .option("checkpointLocation", "shadrin_data_kafka_checkpoint") \
...             .start()
... 
>>> stream = kafka_sink(parsed_dat, 5)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'parsed_dat' is not defined
>>> stream = kafka_sink(parsed_data, 5)
>>> stream.stop()
>>> def kafka_sink_json(df, freq):
...     return df.selectExpr("CAST(null AS STRING) as key", "CAST(to_json(struct(*)) AS STRING) as value") \
...             .writeStream \
...             .format("kafka") \
...             .trigger(processingTime='%s seconds' % freq) \
...             .option("topic", "shadrin_data_sink") \
...             .option("kafka.bootstrap.servers", kafka_brokers) \
...             .option("checkpointLocation", "shadrin_data_kafka_checkpoint") \
...             .start()
... 
>>> stream = kafka_sink_json(parsed_data, 5)
>>> stream.stop()
>>> 22/01/16 09:59:55 WARN clients.NetworkClient: [Producer clientId=producer-2] Error while fetching metadata with correlation id 19 : {shadrin_data_sink=LEADER_NOT_AVAILABLE}

Traceback (most recent call last):
  File "/opt/spark-2.4.8/python/pyspark/context.py", line 270, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>> extended_data = parsed_data.withColumn("my_current_time", F.current_timestamp())
>>> extended_data.printSchema()
root
 |-- time_id: string (nullable = true)
 |-- ping_ms: float (nullable = true)
 |-- temperature_c: float (nullable = true)
 |-- humidity_p: float (nullable = true)
 |-- offset: long (nullable = true)
 |-- my_current_time: timestamp (nullable = false)

>>> def foreach_batch_sink(df, freq):
...     return df \
...         .writeStream \
...         .foreachBatch(foreach_batch_function) \
...         .trigger(processingTime='%s seconds' % freq) \
...         .start()
... 
>>> def foreach_batch_function(df, epoch_id):
...     print("starting epoch " + str(epoch_id))
...     print("averege values for batch:")
...     df.groupBy("species").avg().show()
...     print("finishing epoch " + str(epoch_id))
... 
>>> stream = foreach_batch_sink(extended_iris, 5)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'extended_iris' is not defined
>>> stream = foreach_batch_sink(extended_data, 5)
>>> starting epoch 0
averege values for batch:
22/01/16 10:10:57 ERROR streaming.MicroBatchExecution: Query [id = 562e3750-3bfe-47b3-b022-9155753b7151, runId = 0a4cfadd-dd44-4793-a7be-ab6aadd4e208] terminated with error
py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "/opt/spark-2.4.8/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 2381, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/opt/spark-2.4.8/python/pyspark/sql/utils.py", line 191, in call
    raise e
AnalysisException: u"cannot resolve '`species`' given input columns: [offset, humidity_p, my_current_time, temperature_c, time_id, ping_ms];;\n'Aggregate ['species], ['species, avg(cast(ping_ms#1376 as double)) AS avg(ping_ms)#1391, avg(cast(temperature_c#1377 as double)) AS avg(temperature_c)#1392, avg(cast(humidity_p#1378 as double)) AS avg(humidity_p)#1393, avg(offset#1379L) AS avg(offset)#1394]\n+- SerializeFromObject [if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, time_id), StringType), true, false) AS time_id#1375, if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, ping_ms), FloatType) AS ping_ms#1376, if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, temperature_c), FloatType) AS temperature_c#1377, if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 3, humidity_p), FloatType) AS humidity_p#1378, if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 4, offset), LongType) AS offset#1379L, staticinvoke(class org.apache.spark.sql.catalyst.util.DateTimeUtils$, TimestampType, fromJavaTimestamp, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 5, my_current_time), TimestampType), true, false) AS my_current_time#1380]\n   +- ExternalRDD [obj#1374]\n"

	at py4j.Protocol.getReturnValue(Protocol.java:473)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)
	at com.sun.proxy.$Proxy29.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$$anonfun$callForeachBatch$1.apply(ForeachBatchSink.scala:55)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$$anonfun$callForeachBatch$1.apply(ForeachBatchSink.scala:55)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5$$anonfun$apply$19.apply(MicroBatchExecution.scala:548)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5.apply(MicroBatchExecution.scala:546)
	at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch(MicroBatchExecution.scala:545)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:198)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)
	at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:166)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:160)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:281)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:193)
>>> stream.stop()
>>> exit()
[student898_2@bigdataanalytics-worker-3 ~]$ logout
Connection to 37.139.41.176 closed.
igor@igor-MS-7808:~$ 

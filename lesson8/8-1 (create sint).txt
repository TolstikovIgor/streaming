#export PYSPARK_PYTHON=python3
#export SPARK_KAFKA_VERSION=0.10
# pyspark --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5,com.datastax.spark:spark-cassandra-connector_2.11:2.4.2 --driver-memory 512m --num-executors 1 --executor-memory 512m
from pyspark.ml import Pipeline, PipelineModel
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.types import StructType, StringType, IntegerType, TimestampType
from pyspark.sql import functions as F
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import OneHotEncoderEstimator, VectorAssembler, CountVectorizer, StringIndexer, IndexToString


spark = SparkSession.builder.appName("danilov_spark").getOrCreate()


#####################################
def console_output(df):
    return df.writeStream.format("console").trigger(processingTime='30 seconds').start()

####create new SINT data
my_df = spark.createDataFrame( range( 1 , 200000 ), IntegerType())
items_df = my_df.select(F.col("value").alias("order_id"), \
                        F.round( (F.rand()*49999)+1 ).alias("user_id").cast("integer"), \
                        F.round( (F.rand()*9)+1).alias("items_count").cast("integer")). \
    withColumn("price", (F.col("items_count")* F.round( (F.rand()*999)+1)).cast("integer") ). \
    withColumn("order_date", F.from_unixtime(F.unix_timestamp(F.current_date()) + (F.lit(F.col("order_id")*10))))

items_df.write.format("parquet").option("path", "parquet_data/sales").saveAsTable("sint_sales.sales", mode="overwrite")

items_df=spark.table("sint_sales.sales")

###

spark.sql("""create table sint_sales.users
(user_id int,
gender string,
age string,
segment string)
stored as parquet location 'parquet_data/users'""")

spark.sql("""insert into sint_sales.users
select user_id, case when pmod( user_id, 2 )=0 then 'M' else 'F' end, 
case when pmod(user_id, 3 )=0 then 'young' when pmod(user_id, 3 )=1 then 'midage' else 'old' end ,
case when s>23 then 'happy' when s>15 then 'neutral' else 'shy' end
from (
select sum(items_count) s, user_id from sint_sales.sales group by user_id ) t""")


spark.sql("""create table sint_sales.users_known stored as parquet location 'parquet_data/users_known' as 
select * from sint_sales.users where user_id < 30000
""")

spark.sql("""create table sint_sales.users_unknown stored as parquet location 'parquet_data/users_unknown' as 
select user_id, gender, age from sint_sales.users where user_id >= 30000
""")

spark.sql("""create table sint_sales.sales_known stored as parquet location 'parquet_data/sales_known' as
select * from sint_sales.sales where user_id < 30000
""")

spark.sql("""create table sint_sales.sales_unknown stored as parquet location 'parquet_data/sales_unknown' as
select * from sint_sales.sales where user_id >= 30000
""")


items_df=spark.table("sint_sales.sales")

df = spark.sql("""
select count(*) as c, sum(items_count) as s1, max(items_count) as ma1, min(items_count) as mi1,
sum(price) as s2, max(price) as ma2, min(price) as mi2 ,u.gender, u.age, u.user_id, u.segment 
from sint_sales.sales_known s join sint_sales.users_known u 
where s.user_id = u.user_id 
group by u.user_id, u.gender, u.age, u.segment""")


spark.conf.set("spark.cassandra.connection.host", "bigdataanalytics-worker-2.novalocal")
spark.conf.set("spark.cassandra.output.consistency.level", "ANY")
spark.conf.set("spark.cassandra.input.consistency.level", "ONE")


CASSANDRA:

 CREATE TABLE users_unknown( user_id int, gender text, age text, c int, s1 int,  ma1 int, mi1 int, s2 int, ma2 int, mi2 int, segment text, primary key (user_id));

cass_df = spark.read \
    .format("org.apache.spark.sql.cassandra") \
    .options(table="users_unknown", keyspace="keyspace1") \
    .load()
cass_df.show()

#######

sales_unknown = spark.table("sint_sales.sales_unknown")
sales_unknown = spark.read.parquet("/apps/spark/warehouse/sint_sales.db/sales_unknown")

kafka_brokers = '127.0.0.1:2181'
sales_unknown.selectExpr("cast (null as string) as key", "cast (to_json(struct(*)) as string) as value"). \
    write.format("kafka"). \
    option("kafka.bootstrap.servers", kafka_brokers). \
    option("topic", "sales_unknown"). \
    save()

users_unknown = spark.table("sint_sales.users_unknown")

spark.conf.set("spark.cassandra.connection.host", "89.208.230.116:9042")
spark.conf.set("spark.cassandra.output.consistency.level", "ANY")
spark.conf.set("spark.cassandra.input.consistency.level", "ONE")

users_unknown.write \
    .format("org.apache.spark.sql.cassandra") \
    .options(table="users_unknown", keyspace="keyspace1") \
    .mode("append")\
    .save()

